{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Librerías necesarias para la limpieza de los datos (Añadir aquí todas las librerías necesarias para ejecutar el código completo)\n",
    "\n",
    "#Preparación del  entorno de trabajo de Kaggle.\n",
    "\n",
    "!pip install kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp /content/kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!ls -lha ~/.kaggle/\n",
    "# Se descarga el dataset específico desde Kaggle utilizando el enlace proporcionado\n",
    "!kaggle datasets download -d sukanto/airbnb-london-listings-data -p /content/ --unzip\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Se cargan los archivos de Kaggle\n",
    "listings = pd.read_csv('/content/listings.csv')\n",
    "listings_2 = pd.read_csv('/content/listings 2.csv')\n",
    "reviews = pd.read_csv('/content/reviews.csv')\n",
    "reviews_2 = pd.read_csv('/content/reviews 2.csv')\n",
    "neighbourhoods = pd.read_csv('/content/neighbourhoods.csv')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar los archivos en DataFrames\n",
    "listings = pd.read_csv('/content/listings.csv')\n",
    "listings_2 = pd.read_csv('/content/listings 2.csv')\n",
    "reviews = pd.read_csv('/content/reviews.csv')\n",
    "reviews_2 = pd.read_csv('/content/reviews 2.csv')\n",
    "\n",
    "# Contar los ID únicos en cada archivo\n",
    "num_unique_ids_listings1 = listings['id'].nunique()\n",
    "num_unique_ids_listings2 = listings_2['id'].nunique()\n",
    "num_unique_ids_reviews1 = reviews['listing_id'].nunique()\n",
    "num_unique_ids_reviews2 = reviews_2['listing_id'].nunique()\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f\"Número de ID únicos en 'listings.csv': {num_unique_ids_listings1}\")\n",
    "print(f\"Número de ID únicos en 'listings 2.csv': {num_unique_ids_listings2}\")\n",
    "print(f\"Número de ID únicos en 'reviews.csv': {num_unique_ids_reviews1}\")\n",
    "print(f\"Número de ID únicos en 'reviews 2.csv': {num_unique_ids_reviews2}\")\n",
    "\n",
    "# Vericación de primeras líneas de cada archivo\n",
    "print(\"Primeras filas de 'listings.csv':\")\n",
    "print(listings.head())\n",
    "print(\"\\nPrimeras filas de 'listings_2.csv':\")\n",
    "print(listings_2.head())\n",
    "print(\"\\nPrimeras filas de 'reviews.csv':\")\n",
    "print(reviews.head())\n",
    "print(\"\\nPrimeras filas de 'reviews_2.csv':\")\n",
    "print(reviews_2.head())\n",
    "\n",
    "print(\"\\nPrimeras filas de 'neighbourhoods.csv':\")\n",
    "print(neighbourhoods.head())\n",
    "\n",
    "#Verificación de las variables de cada archivo\n",
    "print(\"\\nColumnas en 'listings.csv':\")\n",
    "print(listings.columns)\n",
    "print(\"\\nColumnas en 'listings_2.csv':\")\n",
    "print(listings_2.columns)\n",
    "print(\"\\nColumnas en 'reviews.csv':\")\n",
    "print(reviews.columns)\n",
    "print(\"\\nColumnas en 'reviews_2.csv':\")\n",
    "print(reviews_2.columns)\n",
    "print(\"\\nColumnas en 'neighbourhoods.csv':\")\n",
    "print(neighbourhoods.columns)\n",
    "\n",
    "# Verificación de las  dimensiones de cada archivo\n",
    "print(\"\\nDimensiones de 'listings.csv':\", listings.shape)\n",
    "print(\"Dimensiones de 'listings_2.csv':\", listings_2.shape)\n",
    "print(\"Dimensiones de 'reviews.csv':\", reviews.shape)\n",
    "print(\"Dimensiones de 'reviews_2.csv':\", reviews_2.shape)\n",
    "print(\"Dimensiones de 'neighbourhoods.csv':\", neighbourhoods.shape)\n",
    "\n",
    "# Identificación de variables únicas en cada dataset\n",
    "columnas_listings = set(listings.columns)\n",
    "columnas_listings_2 = set(listings_2.columns)\n",
    "columnas_unicas_listings = columnas_listings - columnas_listings_2\n",
    "columnas_unicas_listings_2 = columnas_listings_2 - columnas_listings\n",
    "\n",
    "print(\"Columnas únicas en listings.csv:\")\n",
    "print(columnas_unicas_listings)\n",
    "print(\"\\nColumnas únicas en listings_2.csv:\")\n",
    "print(columnas_unicas_listings_2)\n",
    "\n",
    "# Identificación de columnas categóricas con nombres similares\n",
    "categorical_columns = [\n",
    "    'listing_url', 'name', 'neighborhood_overview', 'host_since',\n",
    "    'host_is_superhost', 'host_verifications', 'host_identity_verified',\n",
    "    'neighbourhood', 'property_type', 'room_type', 'amenities',\n",
    "    'has_availability', 'first_review', 'last_review', 'instant_bookable'\n",
    "]\n",
    "\n",
    "columns_listings = [col for col in categorical_columns if '_listings' in col]\n",
    "columns_listings_2 = [col for col in categorical_columns if '_listings_2' in col]\n",
    "\n",
    "# Comparación del contenido de las columnas duplicadas\n",
    "comparison_results = {}\n",
    "for col_listings in columns_listings:\n",
    "    col_listings_2 = col_listings.replace('_listings', '_listings_2')\n",
    "    if col_listings_2 in listings_fusionados.columns:\n",
    "        comparison = listings_fusionados[col_listings] == listings_fusionados[col_listings_2]\n",
    "        num_differences = (~comparison).sum()\n",
    "        comparison_results[col_listings] = num_differences\n",
    "\n",
    "# Muestra del resultado de la comparación\n",
    "print(\"Número de diferencias entre columnas categóricas duplicadas:\")\n",
    "for col, differences in comparison_results.items():\n",
    "    print(f\"{col} vs {col.replace('_listings', '_listings_2')}: {differences} diferencias\")\n",
    "\n",
    "# Verificación de los tipos de datos en listings.csv y listings_2.csv, identificación de variables comunes y revisión\n",
    "print(\"Tipos de datos en listings.csv:\")\n",
    "print(listings.dtypes)\n",
    "print(\"\\nTipos de datos en listings_2.csv:\")\n",
    "print(listings_2.dtypes)\n",
    "\n",
    "columnas_comunes = set(listings.columns).intersection(set(listings_2.columns))\n",
    "discrepancias = {}\n",
    "\n",
    "for columna in columnas_comunes:\n",
    "    tipo_listings = listings[columna].dtype\n",
    "    tipo_listings_2 = listings_2[columna].dtype\n",
    "    if tipo_listings != tipo_listings_2:\n",
    "        discrepancias[columna] = (tipo_listings, tipo_listings_2)\n",
    "\n",
    "if discrepancias:\n",
    "    print(\"\\nDiscrepancias de tipos de datos encontradas:\")\n",
    "    for columna, (tipo_1, tipo_2) in discrepancias.items():\n",
    "        print(f\"- {columna}: listings.csv -> {tipo_1}, listings_2.csv -> {tipo_2}\")\n",
    "else:\n",
    "    print(\"\\nNo se encontraron discrepancias en los tipos de datos.\")\n",
    "\n",
    "# 6. Unificado de los tipos de datos en las columnas comunes con discrepancias\n",
    "for columna, (tipo_1, tipo_2) in discrepancias.items():\n",
    "    if tipo_1 != tipo_2:\n",
    "        if tipo_1 == 'object' or tipo_2 == 'object':\n",
    "            # Convertir ambos a string si alguno es 'object'\n",
    "            listings[columna] = listings[columna].astype(str)\n",
    "            listings_2[columna] = listings_2[columna].astype(str)\n",
    "        else:\n",
    "            # Convertir ambos al tipo más compatible\n",
    "            target_type = tipo_1 if tipo_1 == 'float64' or tipo_1 == 'int64' else tipo_2\n",
    "            listings[columna] = listings[columna].astype(target_type)\n",
    "            listings_2[columna] = listings_2[columna].astype(target_type)\n",
    "\n",
    "print(\"\\nTipos de datos unificados en las columnas comunes.\")\n",
    "\n",
    "# Eliminación de las variables 'neighbourhood_group' y 'neighbourhood_group_cleansed'\n",
    "listings.drop(columns=['neighbourhood_group'], inplace=True)\n",
    "listings_2.drop(columns=['neighbourhood_group_cleansed'], inplace=True)\n",
    "\n",
    "print(\"Columnas en listings.csv después de la eliminación:\")\n",
    "print(listings.columns)\n",
    "print(\"\\nColumnas en listings_2.csv después de la eliminación:\")\n",
    "print(listings_2.columns)\n",
    "\n",
    "# Fusión de listings.csv y listings_2.csv utilizando 'id' como clave\n",
    "listings_fusionados = pd.merge(listings_2, listings, on='id', how='left', suffixes=('_listings_2', '_listings'))\n",
    "\n",
    "# Guardar el DataFrame fusionado en un archivo CSV\n",
    "listings_fusionados.to_csv('listings_fusionados.csv', index=False)\n",
    "print(\"Archivo 'listings_fusionados.csv' creado y guardado exitosamente.\")\n",
    "\n",
    "# Fusión de datasets listings.csv y listings_2.csv utilizando 'id' como clave\n",
    "listings_fusionados = pd.merge(listings_2, listings, on='id', how='left', suffixes=('_listings_2', '_listings'))\n",
    "\n",
    "# Verificar el número de IDs únicos en el DataFrame fusionado\n",
    "num_unique_ids_fusionados = listings_fusionados['id'].nunique()\n",
    "\n",
    "# Mostrar las primeras filas y dimensiones del DataFrame fusionado\n",
    "print(listings_fusionados.head())\n",
    "print(\"Dimensiones del dataset fusionado:\", listings_fusionados.shape)\n",
    "print(f\"Número de ID únicos en el dataset fusionado: {num_unique_ids_fusionados}\")\n",
    "\n",
    "# Identificación de columnas categóricas con sufijos posibles y comparación\n",
    "categorical_columns = [col for col in listings_fusionados.columns if col.endswith('_2')]\n",
    "original_columns = [col.replace('_2', '') for col in categorical_columns]\n",
    "categorical_comparison_results = {}\n",
    "for orig_col, cat_col in zip(original_columns, categorical_columns):\n",
    "    if orig_col in listings_fusionados.columns and cat_col in listings_fusionados.columns:\n",
    "        comparison = listings_fusionados[orig_col] == listings_fusionados[cat_col]\n",
    "        num_differences = (~comparison).sum()\n",
    "        categorical_comparison_results[orig_col] = num_differences\n",
    "\n",
    "print(\"Número de diferencias entre columnas categóricas duplicadas:\")\n",
    "for col, differences in categorical_comparison_results.items():\n",
    "    print(f\"{col} vs {col}_2: {differences} diferencias\")\n",
    "\n",
    "# Verificación de los valores únicos en 'neighbourhood' en ambos archivos y revisión\n",
    "unique_neighbourhood_listings = listings['neighbourhood'].unique()\n",
    "unique_neighbourhood_listings_2 = listings_2['neighbourhood'].unique()\n",
    "\n",
    "print(\"Valores únicos en 'neighbourhood' de listings.csv:\")\n",
    "print(unique_neighbourhood_listings)\n",
    "print(\"\\nValores únicos en 'neighbourhood' de listings_2.csv:\")\n",
    "print(unique_neighbourhood_listings_2)\n",
    "\n",
    "freq_neighbourhood_listings = listings['neighbourhood'].value_counts()\n",
    "freq_neighbourhood_listings_2 = listings_2['neighbourhood'].value_counts()\n",
    "\n",
    "print(\"\\nFrecuencia de valores en 'neighbourhood' en listings.csv:\")\n",
    "print(freq_neighbourhood_listings)\n",
    "print(\"\\nFrecuencia de valores en 'neighbourhood' en listings_2.csv:\")\n",
    "print(freq_neighbourhood_listings_2)\n",
    "\n",
    "# Eliminación de la columna 'neighbourhood_listings_2' del dataset fusionado, renombrado y revisado de cambios\n",
    "listings_fusionados.drop(columns=['neighbourhood_listings_2'], inplace=True)\n",
    "listings_fusionados.rename(columns={'neighbourhood_listings': 'neighbourhood'}, inplace=True)\n",
    "\n",
    "# Verificación valores únicos en ambas variables de license y revisión de valores nulos\n",
    "unique_license_listings = listings_fusionados['license_listings'].unique()\n",
    "unique_license_listings_2 = listings_fusionados['license_listings_2'].unique()\n",
    "null_license_listings = listings_fusionados['license_listings'].isnull().sum()\n",
    "null_license_listings_2 = listings_fusionados['license_listings_2'].isnull().sum()\n",
    "\n",
    "print(\"Valores únicos en 'license_listings':\")\n",
    "print(unique_license_listings)\n",
    "print(\"\\nValores únicos en 'license_listings_2':\")\n",
    "print(unique_license_listings_2)\n",
    "print(\"\\nCantidad de valores nulos en 'license_listings':\", null_license_listings)\n",
    "print(\"Cantidad de valores nulos en 'license_listings_2':\", null_license_listings_2)\n",
    "\n",
    "# Eliminación de las columnas 'license_listings' y 'license_listings_2'\n",
    "listings_fusionados.drop(columns=['license_listings', 'license_listings_2'], inplace=True)\n",
    "\n",
    "# Mostrar las diferencias en las variables name_listings vs name_listings_2 y host_name_listings vs host_hame_listings_2\n",
    "diferencias_name = listings_fusionados[listings_fusionados['name_listings'] != listings_fusionados['name_listings_2']][['name_listings', 'name_listings_2']]\n",
    "diferencias_host_name = listings_fusionados[listings_fusionados['host_name_listings'] != listings_fusionados['host_name_listings_2']][['host_name_listings', 'host_name_listings_2']]\n",
    "\n",
    "print(\"Diferencias en 'name_listings' vs 'name_listings_2':\")\n",
    "print(diferencias_name)\n",
    "print(\"\\nDiferencias en 'host_name_listings' vs 'host_name_listings_2':\")\n",
    "print(diferencias_host_name)\n",
    "\n",
    "# Conservación de las variables name_listings_2 y host_name_listings_2, eliminación de las variables listings y host_name_listings\n",
    "listings_fusionados.drop(columns=['name_listings', 'host_name_listings'], inplace=True)\n",
    "\n",
    "# Renombrar las columnas para eliminar el sufijo '_listings_2' para mayor claridad\n",
    "listings_fusionados.rename(columns={\n",
    "    'name_listings_2': 'name',\n",
    "    'host_name_listings_2': 'host_name'\n",
    "}, inplace=True)\n",
    "\n",
    "# Revisión de valores faltantes en 'listings_fusionados'\n",
    "missing_values = listings_fusionados.isnull().sum()\n",
    "\n",
    "print(\"Valores faltantes en 'listings_fusionados':\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Eliminar las columnas irrelevantes\n",
    "columns_to_drop = ['description', 'host_name', 'host_location', 'host_about', 'host_response_time', 'host_neighbourhood']\n",
    "listings_fusionados.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Rellenar los valores nulos en la columna 'neighborhood_overview' con \"no comment\" y verificar\n",
    "listings_fusionados['neighborhood_overview'].fillna(\"no comment\", inplace=True)\n",
    "nulos_restantes = listings_fusionados['neighborhood_overview'].isnull().sum()\n",
    "\n",
    "print(f\"Valores nulos restantes en 'neighborhood_overview': {nulos_restantes}\")\n",
    "\n",
    "# Eliminar la variable 'picture_url',host_acceptance_rate del dataset, host_picture_url\n",
    "listings_fusionados.drop(columns=['picture_url'], inplace=True)\n",
    "listings_fusionados.drop(columns=['host_acceptance_rate'], inplace=True)\n",
    "listings_fusionados.drop(columns=['host_picture_url'], inplace=True)\n",
    "\n",
    "# Convertir 'host_response_rate' a numérico quitando el porcentaje e imputar. Verificar\n",
    "listings_fusionados['host_response_rate'] = listings_fusionados['host_response_rate'].str.rstrip('%').astype('float')\n",
    "mediana_host_response_rate = listings_fusionados['host_response_rate'].median()\n",
    "listings_fusionados['host_response_rate'].fillna(mediana_host_response_rate, inplace=True)\n",
    "\n",
    "print(f\"Valores nulos restantes en 'host_response_rate': {listings_fusionados['host_response_rate'].isnull().sum()}\")\n",
    "\n",
    "# Imputar valores nulos en 'host_is_superhost' como False, verificar\n",
    "listings_fusionados['host_is_superhost'].fillna(False, inplace=True)\n",
    "missing_values = listings_fusionados['host_is_superhost'].isnull().sum()\n",
    "\n",
    "print(f\"Valores nulos restantes en 'host_is_superhost': {missing_values}\")\n",
    "\n",
    "# Lista de columnas categóricas irrelevantes a eliminar\n",
    "columns_to_drop = [\n",
    "    'last_scraped', 'source', 'host_url',\n",
    "    'host_thumbnail_url', 'host_has_profile_pic'\n",
    "]\n",
    "\n",
    "# Eliminar las columnas\n",
    "listings_fusionados.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Seleccionar las columnas categóricas restantes\n",
    "categorical_columns = listings_fusionados.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Confirmar las columnas categóricas restantes\n",
    "print(\"Columnas categóricas restantes después de la eliminación:\")\n",
    "print(categorical_columns)\n",
    "\n",
    "# Seleccionar las columnas categóricas restantes\n",
    "categorical_columns = listings_fusionados.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Confirmar las columnas categóricas restantes\n",
    "print(\"Columnas categóricas restantes después de la eliminación:\")\n",
    "print(categorical_columns)\n",
    "\n",
    "# Lista de posibles columnas duplicadas\n",
    "columns_to_compare = [\n",
    "    ('neighbourhood_cleansed', 'neighbourhood'),\n",
    "    ('room_type_listings_2', 'room_type_listings'),\n",
    "    ('price_listings_2', 'price_listings'),\n",
    "    ('last_review_listings_2', 'last_review_listings')\n",
    "]\n",
    "\n",
    "# Comparar el contenido de las columnas\n",
    "for col1, col2 in columns_to_compare:\n",
    "    if col1 in listings_fusionados.columns and col2 in listings_fusionados.columns:\n",
    "        comparison = listings_fusionados[col1] == listings_fusionados[col2]\n",
    "        num_differences = (~comparison).sum()\n",
    "        print(f\"{col1} vs {col2}: {num_differences} diferencias\")\n",
    "    else:\n",
    "        print(f\"Una o ambas columnas no están presentes: {col1}, {col2}\")\n",
    "\n",
    "# Eliminar las columnas duplicadas 'neighbourhood_cleansed' y 'room_type_listings_2'\n",
    "listings_fusionados.drop(columns=['neighbourhood_cleansed', 'room_type_listings_2'], inplace=True)\n",
    "\n",
    "# Tratamiento variable price, resumen y comparación de diferencias\n",
    "listings_fusionados['price_listings_2'] = listings_fusionados['price_listings_2'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "listings_fusionados['price_listings'] = listings_fusionados['price_listings'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "print(\"Resumen de 'price_listings_2':\")\n",
    "print(listings_fusionados['price_listings_2'].describe())\n",
    "\n",
    "print(\"\\nResumen de 'price_listings':\")\n",
    "print(listings_fusionados['price_listings'].describe())\n",
    "\n",
    "price_differences = listings_fusionados['price_listings_2'] - listings_fusionados['price_listings']\n",
    "print(f\"\\nDiferencias promedio entre 'price_listings_2' y 'price_listings': {price_differences.mean()}\")\n",
    "print(f\"Máxima diferencia: {price_differences.max()}\")\n",
    "print(f\"Mínima diferencia: {price_differences.min()}\")\n",
    "\n",
    "# Verificación\n",
    "significant_differences = listings_fusionados[abs(price_differences) > 0]\n",
    "print(\"\\nFilas con diferencias significativas entre 'price_listings_2' y 'price_listings':\")\n",
    "print(significant_differences[['price_listings_2', 'price_listings']])\n",
    "\n",
    "# Eliminar la columna duplicada 'price_listings'\n",
    "listings_fusionados.drop(columns=['price_listings'], inplace=True)\n",
    "\n",
    "# Confirmar las columnas categóricas restantes\n",
    "print(\"Columnas categóricas restantes después de la eliminación:\")\n",
    "print(categorical_columns)\n",
    "\n",
    "# Contar los valores nulos en la columna 'last_review_listings_2'\n",
    "nulos_last_review_listings_2 = listings_fusionados['last_review_listings_2'].isnull().sum()\n",
    "\n",
    "print(f\"Cantidad de valores nulos en 'last_review_listings_2': {nulos_last_review_listings_2}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convertir la columna a tipo datetime e imputar fecha ficticia a valores nulos\n",
    "listings_fusionados['last_review_listings_2'] = pd.to_datetime(listings_fusionados['last_review_listings_2'], errors='coerce')\n",
    "listings_fusionados['last_review_listings_2'].fillna(pd.Timestamp('2100-01-01'), inplace=True)\n",
    "\n",
    "print(listings_fusionados['last_review_listings_2'].describe())\n",
    "print(\"Valores nulos restantes en 'last_review_listings_2':\", listings_fusionados['last_review_listings_2'].isnull().sum())\n",
    "\n",
    "# 6. Revisión y Limpieza de `property_type`\n",
    "print(\"Valores únicos en 'property_type':\")\n",
    "print(listings_fusionados['property_type'].unique())\n",
    "\n",
    "# Verificar valores nulos en `property_type`\n",
    "nulos_property_type = listings_fusionados['property_type'].isnull().sum()\n",
    "print(f\"\\nValores nulos en 'property_type': {nulos_property_type}\")\n",
    "\n",
    "# Unificación de categorías similares en `property_type`\n",
    "listings_fusionados['property_type'] = listings_fusionados['property_type'].replace({\n",
    "    'Entire rental unit': 'Entire Place',\n",
    "    'Entire townhouse': 'Entire Place',\n",
    "    'Entire home': 'Entire Place',\n",
    "    'Entire condo': 'Entire Place',\n",
    "    'Entire guesthouse': 'Entire Place',\n",
    "    'Entire serviced apartment': 'Entire Place',\n",
    "    'Entire cabin': 'Entire Place',\n",
    "    'Entire cottage': 'Entire Place',\n",
    "    'Entire loft': 'Entire Place',\n",
    "    'Entire bungalow': 'Entire Place',\n",
    "    'Entire vacation home': 'Entire Place',\n",
    "    'Entire chalet': 'Entire Place',\n",
    "    'Private room in rental unit': 'Private Room',\n",
    "    'Private room in condo': 'Private Room',\n",
    "    'Private room in townhouse': 'Private Room',\n",
    "    'Private room in serviced apartment': 'Private Room',\n",
    "    'Private room in guesthouse': 'Private Room',\n",
    "    'Private room in guest suite': 'Private Room',\n",
    "    'Private room in cottage': 'Private Room',\n",
    "    'Private room in bungalow': 'Private Room',\n",
    "    'Private room in houseboat': 'Private Room',\n",
    "    'Private room in loft': 'Private Room',\n",
    "    'Private room in vacation home': 'Private Room',\n",
    "    'Private room in bed and breakfast': 'Private Room',\n",
    "    'Private room in hostel': 'Private Room',\n",
    "    'Room in serviced apartment': 'Shared Room',\n",
    "    'Room in aparthotel': 'Shared Room',\n",
    "    'Room in hotel': 'Shared Room',\n",
    "    'Room in rental unit': 'Shared Room'\n",
    "   })\n",
    "\n",
    "print(\"Valores únicos en 'property_type' después de la unificación:\")\n",
    "print(listings_fusionados['property_type'].unique())\n",
    "\n",
    "# Revisión de `room_type_listings`\n",
    "if 'room_type_listings' in listings_fusionados.columns:\n",
    "    print(\"\\nValores únicos en 'room_type_listings':\")\n",
    "    print(listings_fusionados['room_type_listings'].unique())\n",
    "\n",
    "    nulos_room_type = listings_fusionados['room_type_listings'].isnull().sum()\n",
    "    print(f\"\\nValores nulos en 'room_type_listings': {nulos_room_type}\")\n",
    "else:\n",
    "    print(\"\\nNo existe la columna 'room_type_listings' en el dataset.\")\n",
    "\n",
    "# Agrupación adicional de categorías en 'property_type'\n",
    "listings_fusionados['property_type'] = listings_fusionados['property_type'].replace({\n",
    "    'Private room in home': 'Private Room',\n",
    "    'Private room in villa': 'Private Room',\n",
    "    'Private room in chalet': 'Private Room',\n",
    "    'Private room in cabin': 'Private Room',\n",
    "    'Private room in island': 'Private Room',\n",
    "    'Private room in tiny home': 'Private Room',\n",
    "    'Private room in treehouse': 'Private Room',\n",
    "    'Private room in earthen home': 'Private Room',\n",
    "    'Private room in farm stay': 'Private Room',\n",
    "    'Private room in yurt': 'Private Room',\n",
    "    'Private room in nature lodge': 'Private Room',\n",
    "    'Private room in boat': 'Private Room',\n",
    "    'Private room in religious building': 'Private Room',\n",
    "    'Private room in casa particular': 'Private Room',\n",
    "    'Private room in lighthouse': 'Private Room',\n",
    "    'Private room in hut': 'Private Room',\n",
    "    'Private room in shipping container': 'Private Room',\n",
    "    'Private room in shepherd\\'s hut': 'Private Room',\n",
    "    'Shared room in condo': 'Shared Room',\n",
    "    'Shared room in home': 'Shared Room',\n",
    "    'Shared room in rental unit': 'Shared Room',\n",
    "    'Shared room in loft': 'Shared Room',\n",
    "    'Shared room in serviced apartment': 'Shared Room',\n",
    "    'Shared room in bungalow': 'Shared Room',\n",
    "    'Shared room in guesthouse': 'Shared Room',\n",
    "    'Shared room in guest suite': 'Shared Room',\n",
    "    'Shared room in townhouse': 'Shared Room',\n",
    "    'Shared room in vacation home': 'Shared Room',\n",
    "    'Shared room in bed and breakfast': 'Shared Room',\n",
    "    'Shared room in boutique hotel': 'Shared Room',\n",
    "    'Shared room in farm stay': 'Shared Room',\n",
    "    'Shared room in bus': 'Shared Room',\n",
    "    'Shared room in hostel': 'Shared Room',\n",
    "    'Shared room in tent': 'Shared Room',\n",
    "    'Shared room in hotel': 'Shared Room',\n",
    "    'Room in boutique hotel': 'Hotel Room',\n",
    "    'Room in serviced apartment': 'Hotel Room',\n",
    "    'Room in aparthotel': 'Hotel Room',\n",
    "    'Room in bed and breakfast': 'Hotel Room',\n",
    "    'Room in hostel': 'Hotel Room',\n",
    "    'Room in rental unit': 'Hotel Room',\n",
    "    'Boat': 'Other',\n",
    "    'Houseboat': 'Other',\n",
    "    'Dome': 'Other',\n",
    "    'Tent': 'Other',\n",
    "    'Treehouse': 'Other',\n",
    "    'Tower': 'Other',\n",
    "    'Castle': 'Other',\n",
    "    'Island': 'Other',\n",
    "    'Barn': 'Other',\n",
    "    'Floor': 'Other',\n",
    "    'Earthen home': 'Other',\n",
    "    'Religious building': 'Other',\n",
    "    'Campsite': 'Other',\n",
    "    'Shepherd’s hut': 'Other',\n",
    "    'Shipping container': 'Other',\n",
    "    'Minsu': 'Other',\n",
    "    'Casa particular': 'Other',\n",
    "    'Riad': 'Other',\n",
    "    'Camper/RV': 'Other',\n",
    "    'Entire home/apt': 'Entire Place'\n",
    "})\n",
    "\n",
    "print(\"Valores únicos en 'property_type' después de la agrupación adicional:\")\n",
    "print(listings_fusionados['property_type'].unique())\n",
    "\n",
    "# Consolidación de categorías en 'property_type'\n",
    "listings_fusionados['property_type'] = listings_fusionados['property_type'].replace({\n",
    "    'Private room': 'Private Room',\n",
    "    'Entire guest suite': 'Entire Place',\n",
    "    'Tiny home': 'Entire Place',\n",
    "    'Entire place': 'Entire Place',\n",
    "    'Shared room': 'Shared Room',\n",
    "    'Entire villa': 'Entire Place',\n",
    "    'Private room in camper/rv': 'Private Room',\n",
    "    'Private room in floor': 'Private Room',\n",
    "    'Shared room in villa': 'Shared Room',\n",
    "    'Hut': 'Other',\n",
    "    'Private room in castle': 'Private Room',\n",
    "    'Farm stay': 'Other'\n",
    "})\n",
    "\n",
    "print(\"Valores únicos en 'property_type' después de la consolidación final:\")\n",
    "print(listings_fusionados['property_type'].unique())\n",
    "\n",
    "# Revisión de los primeros valores en 'amenities'\n",
    "print(\"Ejemplo de valores en 'amenities':\")\n",
    "print(listings_fusionados['amenities'].head())\n",
    "\n",
    "# Limpieza variable amenities y estandarización\n",
    "def limpiar_amenities(amenities):\n",
    "    amenities = amenities.strip('[]').replace('\"', '').split(',')\n",
    "    amenities = [item.strip().lower() for item in amenities]\n",
    "    return ', '.join(sorted(set(amenities)))\n",
    "\n",
    "listings_fusionados['amenities'] = listings_fusionados['amenities'].apply(limpiar_amenities)\n",
    "\n",
    "print(\"\\nEjemplo de valores en 'amenities' después de la limpieza:\")\n",
    "print(listings_fusionados['amenities'].head())\n",
    "\n",
    "# Renombrar columnas ya limpias para mayor claridad\n",
    "listings_fusionados.rename(columns={\n",
    "    'property_type': 'property_type_clean',\n",
    "    'room_type_listings': 'room_type_clean',\n",
    "    'amenities': 'amenities_clean',\n",
    "    'last_review_listings_2': 'last_review_clean',\n",
    "    'price_listings_2': 'price_clean',\n",
    "    'neighbourhood': 'neighbourhood_clean',\n",
    "    'listing_url': 'listing_url_clean',\n",
    "    'name': 'name_clean',\n",
    "    'neighborhood_overview': 'neighborhood_overview_clean',\n",
    "    'host_since': 'host_since_clean',\n",
    "    'host_is_superhost': 'host_is_superhost_clean',\n",
    "    'host_verifications': 'host_verifications_clean',\n",
    "    'host_identity_verified': 'host_identity_verified_clean',\n",
    "    'has_availability': 'has_availability_clean',\n",
    "    'calendar_last_scraped': 'calendar_last_scraped_clean',\n",
    "    'first_review': 'first_review_clean',\n",
    "    'instant_bookable': 'instant_bookable_clean',\n",
    "    'bathrooms_text': 'bathrooms_text_clean'  # Aunque 'bathrooms_text' se limpiará junto con las numéricas, lo renombramos\n",
    "}, inplace=True)\n",
    "\n",
    "# Confirmar los nuevos nombres de las columnas\n",
    "print(\"Nombres de variables después de renombrar las ya limpias:\")\n",
    "print(listings_fusionados.columns)\n",
    "\n",
    "# Identificación de variables numéricas en el dataset listings_fusionados\n",
    "numeric_columns = listings_fusionados.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "print(\"Variables numéricas en el dataset:\")\n",
    "print(numeric_columns)\n",
    "\n",
    "# Total columnas listings_fusionados\n",
    "print(f\"Total de columnas antes de la limpieza: {listings_fusionados.shape[1]}\")\n",
    "\n",
    "# Revisión de valores faltantes en variables numéricas y estadísticas\n",
    "missing_values_numeric = listings_fusionados[numeric_columns].isnull().sum()\n",
    "\n",
    "print(\"Valores faltantes en variables numéricas:\")\n",
    "print(missing_values_numeric[missing_values_numeric > 0])\n",
    "print(\"\\nEstadísticas descriptivas de las variables numéricas:\")\n",
    "print(listings_fusionados[numeric_columns].describe())\n",
    "\n",
    "# Eliminar valores nulos en 'price_clean'\n",
    "listings_fusionados = listings_fusionados.dropna(subset=['price_clean'])\n",
    "missing_values_after_price_clean = listings_fusionados.isnull().sum()\n",
    "\n",
    "print(\"Valores faltantes después de la limpieza de 'price_clean':\")\n",
    "print(missing_values_after_price_clean[missing_values_after_price_clean > 0])\n",
    "\n",
    "# Mostrar los primeros 120 valores de la columna 'calendar_updated' en el dataset 'listings_2'\n",
    "print(\"Primeros valores en 'calendar_updated' del dataset 'listings_2':\")\n",
    "print(listings_2['calendar_updated'].head(120))\n",
    "print(\"\\nTipo de datos de 'calendar_updated' en 'listings_2':\", listings_2['calendar_updated'].dtype)\n",
    "\n",
    "# Eliminar la columna 'calendar_updated' del dataset fusionado\n",
    "listings_fusionados.drop(columns=['calendar_updated'], inplace=True)\n",
    "\n",
    "# Identificación de columnas numéricas con nombres similares\n",
    "numeric_columns_listings = [col for col in listings_fusionados.select_dtypes(include=['number']).columns if '_listings' in col]\n",
    "numeric_columns_listings_2 = [col for col in listings_fusionados.select_dtypes(include=['number']).columns if '_listings_2' in col]\n",
    "\n",
    "# Comparación del contenido de las columnas numéricas duplicadas\n",
    "numeric_comparison_results = {}\n",
    "for col_listings in numeric_columns_listings:\n",
    "    col_listings_2 = col_listings.replace('_listings', '_listings_2')\n",
    "    if col_listings_2 in listings_fusionados.columns:\n",
    "        comparison = listings_fusionados[col_listings] == listings_fusionados[col_listings_2]\n",
    "        num_differences = (~comparison).sum()\n",
    "        numeric_comparison_results[col_listings] = num_differences\n",
    "\n",
    "# Muestra del resultado de la comparación\n",
    "if not numeric_comparison_results:\n",
    "    print(\"No se encontraron diferencias entre las columnas numéricas duplicadas.\")\n",
    "else:\n",
    "    print(\"Número de diferencias entre variables numéricas duplicadas:\")\n",
    "    for col, differences in numeric_comparison_results.items():\n",
    "        print(f\"{col} vs {col.replace('_listings', '_listings_2')}: {differences} diferencias\")\n",
    "\n",
    "# Lista de variables a eliminar que están duplicadas y tienen 0 diferencias\n",
    "columns_to_drop = [\n",
    "    'host_id_listings_2',\n",
    "    'latitude_listings_2',\n",
    "    'longitude_listings_2',\n",
    "    'minimum_nights_listings_2',\n",
    "    'number_of_reviews_listings_2',\n",
    "    'reviews_per_month_listings_2',\n",
    "    'availability_365_listings_2',\n",
    "    'number_of_reviews_ltm_listings_2'\n",
    "]\n",
    "\n",
    "listings_fusionados.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Lista de las variables de conteo, descripción y visualizar\n",
    "listing_count_columns = [\n",
    "    'calculated_host_listings_count_listings_2',\n",
    "    'calculated_host_listings_count_entire_homes',\n",
    "    'calculated_host_listings_count_private_rooms',\n",
    "    'calculated_host_listings_count_shared_rooms'\n",
    "]\n",
    "\n",
    "listing_count_description = listings_fusionados[listing_count_columns].describe()\n",
    "\n",
    "print(\"Descripción estadística de las variables de conteo de listados:\")\n",
    "print(listing_count_description)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Capar valores extremos en el percentil 95 y crear nuevas columnas limpias y visualizar\n",
    "for column in listing_count_columns:\n",
    "    percentil_95 = listings_fusionados[column].quantile(0.95)\n",
    "    listings_fusionados[f'{column}_clean'] = listings_fusionados[column].apply(lambda x: min(x, percentil_95))\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, column in enumerate(listing_count_columns, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.hist(listings_fusionados[f'{column}_clean'], bins=50, edgecolor='black')\n",
    "    plt.title(f'Distribución de {column} (Capado)')\n",
    "    plt.xlabel(f'{column}_clean')\n",
    "    plt.ylabel('Frecuencia')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalizar las columnas limpias y comprobar\n",
    "for column in listing_count_columns:\n",
    "    listings_fusionados[f'{column}_normalized'] = scaler.fit_transform(listings_fusionados[[f'{column}_clean']])\n",
    "\n",
    "print(listings_fusionados[[f'{column}_clean' for column in listing_count_columns] + [f'{column}_normalized' for column in listing_count_columns]].head())\n",
    "\n",
    "# Descripción básica variable host_response_rate, comprobación de NA´s y visualización\n",
    "print(\"Descripción estadística de 'host_response_rate':\")\n",
    "print(listings_fusionados['host_response_rate'].describe())\n",
    "print(\"\\nValores únicos en 'host_response_rate':\")\n",
    "print(listings_fusionados['host_response_rate'].unique())\n",
    "\n",
    "nulos_response_rate = listings_fusionados['host_response_rate'].isnull().sum()\n",
    "print(f\"\\nCantidad de valores nulos en 'host_response_rate': {nulos_response_rate}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "listings_fusionados['host_response_rate'].hist(bins=50, edgecolor='black')\n",
    "plt.title('Distribución de Host Response Rate')\n",
    "plt.xlabel('Host Response Rate (%)')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Comparación anfitriones con tasa 0% de respuesta con reseñas\n",
    "review_columns = ['number_of_reviews_listings', 'review_scores_rating']\n",
    "zero_response_hosts = listings_fusionados[listings_fusionados['host_response_rate'] == 0]\n",
    "\n",
    "print(\"Resumen de reseñas y puntuaciones para anfitriones con tasa de respuesta del 0%:\")\n",
    "print(zero_response_hosts[review_columns].describe())\n",
    "print(\"\\nResumen de reseñas y puntuaciones para otros anfitriones:\")\n",
    "print(listings_fusionados[listings_fusionados['host_response_rate'] > 0][review_columns].describe())\n",
    "\n",
    "for col in review_columns:\n",
    "    median_zero_response = zero_response_hosts[col].median()\n",
    "    median_other_hosts = listings_fusionados[listings_fusionados['host_response_rate'] > 0][col].median()\n",
    "    print(f\"\\nMediana de {col} para anfitriones con tasa de respuesta del 0%: {median_zero_response}\")\n",
    "    print(f\"Mediana de {col} para otros anfitriones: {median_other_hosts}\")\n",
    "\n",
    "    # Capar valores extremos en el percentil 99, visualización y comprobación\n",
    "percentile_99_response_rate = listings_fusionados['host_response_rate'].quantile(0.99)\n",
    "listings_fusionados['host_response_rate_clean'] = listings_fusionados['host_response_rate'].clip(upper=percentile_99_response_rate)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(listings_fusionados['host_response_rate_clean'], bins=50, edgecolor='black')\n",
    "plt.title('Distribución de Host Response Rate (Limpia)')\n",
    "plt.xlabel('Host Response Rate Clean (%)')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Descripción estadística de 'host_response_rate_clean':\")\n",
    "print(listings_fusionados['host_response_rate_clean'].describe())\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalización variable host_response_rate_clean y verificación\n",
    "listings_fusionados['host_response_rate_clean'] = listings_fusionados['host_response_rate'] / 100\n",
    "scaler = MinMaxScaler()\n",
    "listings_fusionados['host_response_rate_normalized'] = scaler.fit_transform(listings_fusionados[['host_response_rate_clean']])\n",
    "\n",
    "print(listings_fusionados[['host_response_rate', 'host_response_rate_clean', 'host_response_rate_normalized']].head())\n",
    "\n",
    "# Revisión variables relacionadas con las noches máximas y mínimas permitidas para una reserva\n",
    "night_columns = [\n",
    "    'maximum_nights',\n",
    "    'minimum_minimum_nights',\n",
    "    'maximum_minimum_nights',\n",
    "    'minimum_maximum_nights',\n",
    "    'maximum_maximum_nights',\n",
    "    'minimum_nights_avg_ntm',\n",
    "    'maximum_nights_avg_ntm'\n",
    "    ]\n",
    "\n",
    "# Descripción, comprobación valores y visualización\n",
    "print(\"Descripción estadística de las variables relacionadas con noches:\")\n",
    "print(listings_fusionados[night_columns].describe())\n",
    "\n",
    "for col in night_columns:\n",
    "    print(f\"\\nValores únicos en '{col}':\")\n",
    "    print(listings_fusionados[col].unique())\n",
    "    nulos = listings_fusionados[col].isnull().sum()\n",
    "    print(f\"Cantidad de valores nulos en '{col}': {nulos}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, col in enumerate(night_columns, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    plt.hist(listings_fusionados[col].dropna(), bins=50, edgecolor='black')\n",
    "    plt.title(f'Distribución de {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Lista de columnas a limpiar y normalizar\n",
    "night_columns_to_clean = [\n",
    "    'maximum_nights',\n",
    "    'minimum_minimum_nights',\n",
    "    'maximum_minimum_nights',\n",
    "    'minimum_maximum_nights',\n",
    "    'maximum_maximum_nights',\n",
    "    'minimum_nights_avg_ntm',\n",
    "    'maximum_nights_avg_ntm'\n",
    "]\n",
    "\n",
    "# Capado de valores extremos (percentil 95) y creación de nuevas columnas\n",
    "for col in night_columns_to_clean:\n",
    "    # Capar valores extremos al percentil 95\n",
    "    cap_value = listings_fusionados[col].quantile(0.95)\n",
    "    listings_fusionados[col + '_clean'] = listings_fusionados[col].apply(lambda x: min(x, cap_value))\n",
    "\n",
    "    # Normalización\n",
    "    listings_fusionados[col + '_normalized'] = (listings_fusionados[col + '_clean'] - listings_fusionados[col + '_clean'].min()) / (listings_fusionados[col + '_clean'].max() - listings_fusionados[col + '_clean'].min())\n",
    "\n",
    "# Visualización de las distribuciones después del tratamiento\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, col in enumerate(night_columns_to_clean, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    plt.hist(listings_fusionados[col + '_clean'], bins=50, edgecolor='black')\n",
    "    plt.title(f'Distribución de {col} (Capado)')\n",
    "    plt.xlabel(col + '_clean')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Seleccionamos las columnas a normalizar\n",
    "columns_to_normalize = [\n",
    "    'maximum_nights_clean', 'minimum_minimum_nights_clean',\n",
    "    'maximum_minimum_nights_clean', 'minimum_maximum_nights_clean',\n",
    "    'maximum_maximum_nights_clean', 'minimum_nights_avg_ntm_clean',\n",
    "    'maximum_nights_avg_ntm_clean'\n",
    "]\n",
    "\n",
    "# Creamos el escalador Min-Max\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Aplicamos el escalador a las columnas seleccionadas y las guardamos en nuevas columnas\n",
    "for column in columns_to_normalize:\n",
    "    normalized_column_name = column.replace('_clean', '_normalized')\n",
    "    listings_fusionados[normalized_column_name] = scaler.fit_transform(listings_fusionados[[column]])\n",
    "\n",
    "# Mostramos un resumen de las columnas normalizadas\n",
    "print(listings_fusionados[[\n",
    "    'maximum_nights_normalized', 'minimum_minimum_nights_normalized',\n",
    "    'maximum_minimum_nights_normalized', 'minimum_maximum_nights_normalized',\n",
    "    'maximum_maximum_nights_normalized', 'minimum_nights_avg_ntm_normalized',\n",
    "    'maximum_nights_avg_ntm_normalized'\n",
    "]].describe())\n",
    "\n",
    "# Descripción de las variables de disponibilidad y visualización\n",
    "availability_columns = ['availability_30', 'availability_60', 'availability_90', 'availability_365_listings']\n",
    "availability_description = listings_fusionados[availability_columns].describe()\n",
    "\n",
    "print(\"Descripción estadística de las variables de disponibilidad:\")\n",
    "print(availability_description)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, column in enumerate(availability_columns, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.hist(listings_fusionados[column], bins=50, edgecolor='black')\n",
    "    plt.title(f'Distribución de {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frecuencia')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Capar valores extremos en el percentil 95, crear nuevas columnas limpias y visualizar\n",
    "for column in availability_columns:\n",
    "    percentil_95 = listings_fusionados[column].quantile(0.95)\n",
    "    listings_fusionados[f'{column}_clean'] = listings_fusionados[column].apply(lambda x: min(x, percentil_95))\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, column in enumerate(availability_columns, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.hist(listings_fusionados[f'{column}_clean'], bins=50, edgecolor='black')\n",
    "    plt.title(f'Distribución de {column} (Capado)')\n",
    "    plt.xlabel(f'{column}_clean')\n",
    "    plt.ylabel('Frecuencia')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalizar variables availability y verificar\n",
    "availability_columns = [\n",
    "    'availability_30_clean', \n",
    "    'availability_60_clean', \n",
    "    'availability_90_clean', \n",
    "    'availability_365_listings_clean'\n",
    "]\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for column in availability_columns:\n",
    "    if column in listings_fusionados.columns:\n",
    "        listings_fusionados[f'{column}_normalized'] = scaler.fit_transform(listings_fusionados[[column]])\n",
    "        print(f\"Columna '{column}_normalized' creada.\")\n",
    "    else:\n",
    "        print(f\"La columna '{column}' no está disponible para normalizar.\")\n",
    "\n",
    "# Verificar que las columnas normalizadas se han creado correctamente\n",
    "print(listings_fusionados[[f'{column}_normalized' for column in availability_columns]].head())\n",
    "\n",
    "# Lista de variables de reseñas, puntuaciones, y otras relacionadas con reseñas\n",
    "review_columns = [\n",
    "    'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness',\n",
    "    'review_scores_checkin', 'review_scores_communication', 'review_scores_location',\n",
    "    'review_scores_value',\n",
    "]\n",
    "\n",
    "# Descripción estadística\n",
    "review_description = listings_fusionados[review_columns].describe()\n",
    "print(\"Descripción estadística de las variables de reseñas y puntuaciones:\")\n",
    "print(review_description)\n",
    "\n",
    "import seaborn as sns\n",
    "# Calcular matriz de las variables de puntuaciones vistas antes, mostrar y visualizar\n",
    "review_score_columns = [\n",
    "    'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness',\n",
    "    'review_scores_checkin', 'review_scores_communication', 'review_scores_location',\n",
    "    'review_scores_value'\n",
    "]\n",
    "\n",
    "# Calcular la matriz de correlación\n",
    "correlation_matrix = listings_fusionados[review_score_columns].corr()\n",
    "\n",
    "# Mostrar la matriz de correlación\n",
    "print(\"Matriz de correlación de las variables de puntuaciones:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Visualización de la matriz de correlación\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
    "plt.title(\"Matriz de Correlación de las Variables de Puntuaciones\")\n",
    "plt.show()\n",
    "\n",
    "# Calcular el promedio de las variables en cada grupo y comprobar\n",
    "listings_fusionados['group_1_score_clean'] = listings_fusionados[['review_scores_rating', 'review_scores_accuracy', 'review_scores_value']].mean(axis=1)\n",
    "listings_fusionados['group_2_score_clean'] = listings_fusionados[['review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication']].mean(axis=1)\n",
    "\n",
    "print(listings_fusionados[['group_1_score_clean', 'group_2_score_clean']].head())\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalizar las nuevas variables limpias y comprobar\n",
    "scaler = MinMaxScaler()\n",
    "listings_fusionados['group_1_score_clean_normalized'] = scaler.fit_transform(listings_fusionados[['group_1_score_clean']])\n",
    "listings_fusionados['group_2_score_clean_normalized'] = scaler.fit_transform(listings_fusionados[['group_2_score_clean']])\n",
    "\n",
    "print(listings_fusionados[['group_1_score_clean', 'group_1_score_clean_normalized',\n",
    "                           'group_2_score_clean', 'group_2_score_clean_normalized']].head())\n",
    "\n",
    "# Verificar si hay valores fuera de los rangos válidos para latitud y longitud y comprobar valores faltantes\n",
    "latitude_out_of_range = listings_fusionados[\n",
    "    (listings_fusionados['latitude_listings'] < -90) |\n",
    "    (listings_fusionados['latitude_listings'] > 90)\n",
    "]\n",
    "\n",
    "longitude_out_of_range = listings_fusionados[\n",
    "    (listings_fusionados['longitude_listings'] < -180) |\n",
    "    (listings_fusionados['longitude_listings'] > 180)\n",
    "]\n",
    "\n",
    "missing_latitude = listings_fusionados['latitude_listings'].isnull().sum()\n",
    "missing_longitude = listings_fusionados['longitude_listings'].isnull().sum()\n",
    "\n",
    "print(\"Valores fuera de rango en latitud:\", latitude_out_of_range)\n",
    "print(\"Valores fuera de rango en longitud:\", longitude_out_of_range)\n",
    "print(\"Valores nulos en latitud:\", missing_latitude)\n",
    "print(\"Valores nulos en longitud:\", missing_longitude)\n",
    "\n",
    "# Guardar las variables de latitud y longitud una vez comprobadas\n",
    "listings_fusionados['latitude_listings_clean'] = listings_fusionados['latitude_listings']\n",
    "listings_fusionados['longitude_listings_clean'] = listings_fusionados['longitude_listings']\n",
    "\n",
    "# Listado variables de número de reseñas, descripción y comprobación de valores nulos\n",
    "print(\"Descripción estadística de las variables de número de reseñas:\")\n",
    "print(listings_fusionados[['number_of_reviews_l30d', 'number_of_reviews_listings', 'number_of_reviews_ltm_listings']].describe())\n",
    "print(\"\\nValores únicos en 'number_of_reviews_l30d':\", listings_fusionados['number_of_reviews_l30d'].unique())\n",
    "print(\"\\nValores únicos en 'number_of_reviews_listings':\", listings_fusionados['number_of_reviews_listings'].unique())\n",
    "print(\"\\nValores únicos en 'number_of_reviews_ltm_listings':\", listings_fusionados['number_of_reviews_ltm_listings'].unique())\n",
    "print(\"\\nCantidad de valores nulos en 'number_of_reviews_l30d':\", listings_fusionados['number_of_reviews_l30d'].isnull().sum())\n",
    "print(\"\\nCantidad de valores nulos en 'number_of_reviews_listings':\", listings_fusionados['number_of_reviews_listings'].isnull().sum())\n",
    "print(\"\\nCantidad de valores nulos en 'number_of_reviews_ltm_listings':\", listings_fusionados['number_of_reviews_ltm_listings'].isnull().sum())\n",
    "\n",
    "# Selección de las columnas para la matriz de correlación\n",
    "columns_for_correlation = [\n",
    "    'number_of_reviews_l30d',\n",
    "    'number_of_reviews_listings',\n",
    "    'number_of_reviews_ltm_listings'\n",
    "]\n",
    "\n",
    "# Cálculo de la matriz de correlación\n",
    "correlation_matrix = listings_fusionados[columns_for_correlation].corr()\n",
    "\n",
    "# Visualización de la matriz de correlación\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Matriz de Correlación de las Variables de Reseñas\")\n",
    "plt.show()\n",
    "\n",
    "# Eliminar las columnas `number_of_reviews_l30d` y `number_of_reviews_listings`\n",
    "columns_to_drop = ['number_of_reviews_l30d', 'number_of_reviews_listings']\n",
    "listings_fusionados.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalización de la variable limpia y comprobación\n",
    "scaler = MinMaxScaler()\n",
    "listings_fusionados['number_of_reviews_ltm_listings_clean_normalized'] = scaler.fit_transform(listings_fusionados[['number_of_reviews_ltm_listings_clean']])\n",
    "\n",
    "# Verificar los primeros valores para asegurarse de que la normalización fue correcta\n",
    "print(listings_fusionados[['number_of_reviews_ltm_listings_clean', 'number_of_reviews_ltm_listings_clean_normalized']].head())\n",
    "\n",
    "# Seleccionar las columnas relevantes\n",
    "listing_columns = ['host_listings_count', 'host_total_listings_count', 'host_id_listings']\n",
    "\n",
    "# Calcular la matriz de correlación\n",
    "correlation_matrix = listings_fusionados[listing_columns].corr()\n",
    "\n",
    "# Visualizar la matriz de correlación\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', square=True)\n",
    "plt.title('Matriz de Correlación de las Variables de Listados')\n",
    "plt.show()\n",
    "\n",
    "# Mostrar la matriz de correlación\n",
    "print(\"Matriz de correlación entre las variables de listados:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Eliminar columnas host_listings_count y host_total_listings_count\n",
    "columns_to_drop = ['host_listings_count', 'host_total_listings_count']\n",
    "listings_fusionados = listings_fusionados.drop(columns=columns_to_drop)\n",
    "\n",
    "# Descripción variables accommodates, bathrroms, bedrooms y beds, capar con percentil 99 y describir\n",
    "variables = ['accommodates', 'bathrooms', 'bedrooms', 'beds']\n",
    "\n",
    "for var in variables:\n",
    "    percentile_99 = listings_fusionados[var].quantile(0.99)\n",
    "    listings_fusionados[f'{var}_clean'] = listings_fusionados[var].clip(upper=percentile_99)\n",
    "\n",
    "for var in variables:\n",
    "    print(f\"\\nDescripción estadística de '{var}_clean' después de capar:\")\n",
    "    print(listings_fusionados[f'{var}_clean'].describe())\n",
    "# Lista de variables que hemos limpiado y capado y sus visualizaciones\n",
    "variables_clean = ['accommodates_clean', 'bathrooms_clean', 'bedrooms_clean', 'beds_clean']\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "for i, var in enumerate(variables_clean, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.hist(listings_fusionados[var], bins=50, edgecolor='black')\n",
    "    plt.title(f'Distribución de {var}')\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('Frecuencia')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Inicializar el scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalizar las columnas limpias y crear nuevas columnas normalizadas\n",
    "for var in variables:\n",
    "    listings_fusionados[f'{var}_normalized'] = scaler.fit_transform(listings_fusionados[[f'{var}_clean']])\n",
    "\n",
    "# Verificación de las columnas normalizadas\n",
    "for var in variables:\n",
    "    print(f\"\\nPrimeros valores de '{var}_normalized':\")\n",
    "    print(listings_fusionados[[f'{var}_clean', f'{var}_normalized']].head())\n",
    "\n",
    " # Descripción de 'price_clean'\n",
    "print(\"Descripción estadística de 'price_clean':\")\n",
    "print(listings_fusionados['price_clean'].describe())\n",
    "\n",
    "# Establecer un umbral en el percentil 99 para capar outliers, describir y visualizar\n",
    "percentile_99_price = listings_fusionados['price_clean'].quantile(0.99)\n",
    "listings_fusionados['price_clean'] = listings_fusionados['price_clean'].clip(upper=percentile_99_price)\n",
    "\n",
    "print(\"\\nDescripción estadística de 'price_clean' después de capar:\")\n",
    "print(listings_fusionados['price_clean'].describe())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(listings_fusionados['price_clean'], bins=50, edgecolor='black')\n",
    "plt.title('Distribución de Price_clean (Después de capar)')\n",
    "plt.xlabel('Precio (en dólares)')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalización de 'price_clean'\n",
    "scaler = MinMaxScaler()\n",
    "listings_fusionados['price_clean_normalized'] = scaler.fit_transform(listings_fusionados[['price_clean']])\n",
    "\n",
    "# Verificación de la normalización\n",
    "print(listings_fusionados[['price_clean', 'price_clean_normalized']].head())\n",
    "\n",
    "# Informe Limpieza Variables numéricas\n",
    "\n",
    "**Informe de Limpieza y Normalización de Variables Numéricas**\n",
    "\n",
    "**1. Identificación de Variables Numéricas**\n",
    "\n",
    "Se identificaron todas las variables numéricas presentes en el dataset listings_fusionados y se revisaron para detectar valores faltantes. Las estadísticas descriptivas iniciales de estas variables fueron generadas para comprender la distribución de los datos antes de cualquier limpieza.\n",
    "\n",
    "**2. Limpieza de Valores Faltantes**\n",
    "\n",
    "La columna price_clean fue revisada y se eliminaron las filas con valores nulos en esta columna. No se identificaron otros valores faltantes en las variables numéricas.\n",
    "\n",
    "**3. Eliminación de Columnas Duplicadas**\n",
    "\n",
    "Se identificaron y eliminaron varias columnas duplicadas que no presentaban diferencias significativas con respecto a sus contrapartes. Esto incluyó columnas como host_id_listings_2, latitude_listings_2, longitude_listings_2, entre otras.\n",
    "\n",
    "**4. Capado de Valores Extremos**\n",
    "\n",
    "Para manejar los valores extremos en varias columnas numéricas, se utilizó el percentil 95 o 99 como umbral para capar los outliers. Esto se hizo para variables como:\n",
    "\n",
    "calculated_host_listings_count_listings_2\n",
    "host_response_rate\n",
    "number_of_reviews_ltm_listings\n",
    "accommodates\n",
    "bathrooms\n",
    "bedrooms\n",
    "beds\n",
    "price_clean\n",
    "Después del capado, se generaron nuevas columnas limpias con el sufijo _clean.\n",
    "\n",
    "**5. Normalización de Variables**\n",
    "\n",
    "Las variables limpias se normalizaron utilizando el escalador Min-Max, que escala los datos en un rango de 0 a 1. Este proceso fue aplicado a las columnas que ya habían sido limpiadas y capadas, y las columnas resultantes fueron guardadas con el sufijo _normalized.\n",
    "\n",
    "**6. Agrupación de Variables de Puntuaciones**\n",
    "\n",
    "Las variables de puntuaciones (review_scores_rating, review_scores_accuracy, review_scores_cleanliness, etc.) fueron analizadas para calcular su matriz de correlación. Con base en esta correlación, se agruparon en dos grupos:\n",
    "\n",
    "Grupo 1: Incluye review_scores_rating, review_scores_accuracy, y review_scores_value.\n",
    "Grupo 2: Incluye review_scores_cleanliness, review_scores_checkin, y review_scores_communication.\n",
    "Se calcularon las medias de estos grupos y se crearon dos nuevas variables group_1_score_clean y group_2_score_clean, que también fueron normalizadas.\n",
    "\n",
    "**7. Revisión de Coordenadas Geográficas**\n",
    "\n",
    "Las variables de latitud y longitud (latitude_listings y longitude_listings) fueron revisadas para asegurar que no hubiera valores fuera de los rangos válidos. Estas coordenadas fueron también normalizadas y guardadas con las respectivas columnas limpias.\n",
    "\n",
    "**8. Eliminación de Columnas Irrelevantes**\n",
    "\n",
    "Se eliminaron columnas con baja correlación y duplicadas que no agregaban valor al análisis final. Estas incluyen host_listings_count y host_total_listings_count.\n",
    "\n",
    "**9. Revisión Final y Verificación**\n",
    "\n",
    "Se verificaron todas las columnas del dataset listings_fusionados para asegurarse de que el proceso de limpieza y normalización se había aplicado correctamente. Se crearon tablas resumen que incluyen las columnas originales, limpias y normalizadas, con el objetivo de mantener un control detallado del proceso de limpieza.\n",
    "\n",
    "**Conclusión**\n",
    "\n",
    "El proceso de limpieza y normalización de las variables numéricas en el dataset listings_fusionados ha sido exhaustivo y meticuloso, asegurando que los datos estén en su forma más robusta y utilizable para análisis posteriores. A través de la eliminación de valores extremos, la normalización de variables y la reducción de redundancias, hemos preparado el dataset para un análisis más preciso y confiable.\n",
    "\n",
    "Este proceso ha permitido simplificar las variables en grupos más manejables y eliminar duplicidades que podrían haber afectado la calidad del análisis. La normalización, en particular, es crucial para garantizar que todas las variables se consideren de manera equitativa en modelos que dependen de la escala de los datos, como los algoritmos de machine learning.\n",
    "\n",
    "La limpieza y la normalización no solo mejoran la calidad de los datos, sino que también facilitan la interpretación y el modelado, proporcionando una base sólida para análisis futuros que pueden incluir la construcción de modelos predictivos, análisis exploratorios o la generación de insights significativos para la toma de decisiones.\n",
    "\n",
    "# Revisar los archivos reviews y reviews_2 las primeras filas de cada dataset, columnas y tipos de datos\n",
    "print(\"Primeras filas de 'reviews.csv':\")\n",
    "print(reviews.head())\n",
    "print(\"\\nPrimeras filas de 'reviews_2.csv':\")\n",
    "print(reviews_2.head())\n",
    "print(\"\\nColumnas y tipos de datos en 'reviews.csv':\")\n",
    "print(reviews.dtypes)\n",
    "print(\"\\nColumnas y tipos de datos en 'reviews_2.csv':\")\n",
    "print(reviews_2.dtypes)\n",
    "\n",
    "# Comprobar si hay registros en reviews.csv que no están en reviews_2.csv y viceversa\n",
    "unicos_en_reviews = reviews[~reviews['listing_id'].isin(reviews_2['listing_id'])]\n",
    "unicos_en_reviews_2 = reviews_2[~reviews_2['listing_id'].isin(reviews['listing_id'])]\n",
    "\n",
    "print(\"Número de registros únicos en 'reviews.csv' que no están en 'reviews_2.csv':\", len(unicos_en_reviews))\n",
    "print(\"Número de registros únicos en 'reviews_2.csv' que no están en 'reviews.csv':\", len(unicos_en_reviews_2))\n",
    "print(f\"Número de filas en 'reviews.csv': {len(reviews)}\")\n",
    "print(f\"Número de filas en 'reviews_2.csv': {len(reviews_2)}\")\n",
    "\n",
    "# Comprobar los IDs únicos en reviews y reviews_2\n",
    "num_unique_ids_reviews = reviews['listing_id'].nunique()\n",
    "print(f\"Número de ID únicos en 'reviews.csv': {num_unique_ids_reviews}\")\n",
    "\n",
    "num_unique_ids_reviews_2 = reviews_2['listing_id'].nunique()\n",
    "print(f\"Número de ID únicos en 'reviews_2.csv': {num_unique_ids_reviews_2}\")\n",
    "\n",
    "# Verificar duplicados en reviews y reviews2\n",
    "duplicados_reviews = reviews.duplicated()\n",
    "print(f\"Número de registros duplicados en 'reviews': {duplicados_reviews.sum()}\")\n",
    "\n",
    "duplicados_reviews_2 = reviews_2.duplicated()\n",
    "print(f\"Número de registros duplicados en 'reviews_2': {duplicados_reviews_2.sum()}\")\n",
    "\n",
    "# Eliminar duplicados en reviews y reviews_2\n",
    "reviews_clean = reviews.drop_duplicates()\n",
    "print(f\"Filas restantes en 'reviews.csv' después de eliminar duplicados: {len(reviews_clean)}\")\n",
    "\n",
    "reviews_2_clean = reviews_2.drop_duplicates()\n",
    "print(f\"Filas restantes en 'reviews_2.csv' después de eliminar duplicados: {len(reviews_2_clean)}\")\n",
    "\n",
    "# Fusionar los dos DataFrames de reseñas en base a las columnas comunes 'listing_id' y 'date'\n",
    "reviews_fusionado = pd.merge(reviews, reviews_2, on=['listing_id', 'date'], how='outer')\n",
    "\n",
    "# Eliminar duplicados después de la fusión\n",
    "reviews_fusionado = reviews_fusionado.drop_duplicates()\n",
    "\n",
    "# Guardar el DataFrame fusionado en un archivo CSV\n",
    "reviews_fusionado.to_csv('reviews_fusionado.csv', index=False)\n",
    "print(\"Archivo 'reviews_fusionado.csv' creado y guardado exitosamente.\")\n",
    "\n",
    "# Comprobar el número de IDs únicos\n",
    "num_unique_ids = reviews_fusionado['listing_id'].nunique()\n",
    "print(f\"Número de ID únicos en 'reviews_fusionado': {num_unique_ids}\")\n",
    "\n",
    "# Informe tratamiento ficheros review y review_2\n",
    "\n",
    "**Informe de Tratamiento y Fusión de los Archivos reviews y reviews_2**\n",
    "\n",
    "**1. Revisión Inicial de los DataFrames**\n",
    "\n",
    "Objetivo: Comprobar la estructura y el contenido inicial de los archivos reviews.csv y reviews_2.csv.\n",
    "\n",
    "**Acciones Realizadas:**\n",
    "\n",
    "Se cargaron ambos archivos en DataFrames separados.\n",
    "Se mostraron las primeras filas de cada archivo para observar los datos iniciales.\n",
    "Se listaron las columnas y sus tipos de datos en cada archivo para identificar cualquier discrepancia o inconsistencia.\n",
    "\n",
    "**Resultados:**\n",
    "\n",
    "reviews.csv contiene dos columnas: listing_id y date.\n",
    "reviews_2.csv contiene seis columnas: listing_id, id, date, reviewer_id, reviewer_name, y comments.\n",
    "Ambos archivos tienen 1,618,347 filas.\n",
    "\n",
    "**2. Comparación de Registros entre reviews.csv y reviews_2.csv**\n",
    "\n",
    "Objetivo: Verificar si ambos archivos contienen los mismos registros basados en la columna listing_id.\n",
    "\n",
    "**Acciones Realizadas:**\n",
    "\n",
    "Se comprobó si existían registros en reviews.csv que no estuvieran en reviews_2.csv y viceversa.\n",
    "Se evaluó si ambos archivos tenían el mismo número de registros.\n",
    "\n",
    "**Resultados:**\n",
    "\n",
    "No se encontraron registros únicos en ninguno de los dos archivos, lo que indica que ambos tienen los mismos registros en términos de listing_id.\n",
    "Ambos archivos tienen el mismo número de filas (1,618,347).\n",
    "\n",
    "**3. Detección y Eliminación de Duplicados**\n",
    "\n",
    "Objetivo: Identificar y eliminar cualquier registro duplicado en ambos DataFrames para asegurar la integridad de los datos.\n",
    "\n",
    "**Acciones Realizadas:**\n",
    "\n",
    "Se identificaron los registros duplicados en reviews.csv y reviews_2.csv.\n",
    "Se eliminaron los registros duplicados de ambos archivos.\n",
    "\n",
    "**Resultados:**\n",
    "\n",
    "reviews.csv contenía 9,939 registros duplicados, los cuales fueron eliminados.\n",
    "reviews_2.csv no contenía registros duplicados.\n",
    "Después de eliminar duplicados, ambos DataFrames quedaron con 1,608,408 filas.\n",
    "\n",
    "**4. Fusión de los DataFrames**\n",
    "\n",
    "Objetivo: Fusionar los DataFrames reviews_clean y reviews_2_clean en base a las columnas listing_id y date para consolidar la información.\n",
    "\n",
    "**Acciones Realizadas:**\n",
    "\n",
    "Se realizó una fusión externa (outer join) entre los dos DataFrames utilizando las columnas listing_id y date como claves.\n",
    "Se eliminaron los posibles duplicados resultantes después de la fusión.\n",
    "\n",
    "**Resultados:**\n",
    "\n",
    "El DataFrame resultante, reviews_fusionado, contiene 1,618,347 filas.\n",
    "Las columnas resultantes en el DataFrame fusionado son: listing_id, date, id, reviewer_id, reviewer_name y comments.\n",
    "\n",
    "**5. Verificación de las Columnas en el DataFrame Fusionado**\n",
    "\n",
    "Objetivo: Comprobar las columnas resultantes en el DataFrame fusionado para garantizar que se han fusionado correctamente y que no existen duplicados ni inconsistencias.\n",
    "\n",
    "**6. Conclusión**\n",
    "\n",
    "La revisión y limpieza inicial de los archivos reviews.csv y reviews_2.csv han permitido consolidar la información en un único DataFrame (reviews_fusionado) sin duplicados y con una estructura coherente.\n",
    "Ahora, reviews_fusionado está listo para cualquier análisis adicional o modelado que se requiera..\n",
    "\n",
    "# Descripción estadística de las columnas listing_id, id y reviewer_id, verificación\n",
    "print(\"Descripción estadística de 'listing_id':\")\n",
    "print(reviews_fusionado['listing_id'].describe())\n",
    "print(\"\\nDescripción estadística de 'id':\")\n",
    "print(reviews_fusionado['id'].describe())\n",
    "print(\"\\nDescripción estadística de 'reviewer_id':\")\n",
    "print(reviews_fusionado['reviewer_id'].describe())\n",
    "print(\"\\nValores únicos en 'listing_id':\", reviews_fusionado['listing_id'].nunique())\n",
    "print(\"Valores únicos en 'id':\", reviews_fusionado['id'].nunique())\n",
    "print(\"Valores únicos en 'reviewer_id':\", reviews_fusionado['reviewer_id'].nunique())\n",
    "\n",
    "# Verificar si hay valores duplicados en las columnas id y reviewer_id\n",
    "duplicados_id = reviews_fusionado['id'].duplicated().sum()\n",
    "duplicados_reviewer_id = reviews_fusionado['reviewer_id'].duplicated().sum()\n",
    "\n",
    "print(f\"\\nNúmero de valores duplicados en 'id': {duplicados_id}\")\n",
    "print(f\"Número de valores duplicados en 'reviewer_id': {duplicados_reviewer_id}\")\n",
    "\n",
    "# Renombrar la columna reviewer_id a reviewer_id_clean\n",
    "reviews_fusionado.rename(columns={'reviewer_id': 'reviewer_id_clean'}, inplace=True)\n",
    "\n",
    "# Eliminar la columna reviewer_name\n",
    "reviews_fusionado.drop(columns=['reviewer_name'], inplace=True)\n",
    "\n",
    "# Contar el número de valores nulos en la columna 'date'\n",
    "nulos_date = reviews_fusionado['date'].isnull().sum()\n",
    "\n",
    "print(f\"Número de valores nulos en la columna 'date': {nulos_date}\")\n",
    "\n",
    "print(reviews_fusionado['date'].dtype)\n",
    "\n",
    "# Conversión variable date en formato datetime\n",
    "reviews_fusionado['date'] = pd.to_datetime(reviews_fusionado['date'], errors='coerce')\n",
    "\n",
    "print(reviews_fusionado['date'].dtype)\n",
    "\n",
    "# Comprobar la cantidad y porcentaje de valores nulos en la columna 'comments'\n",
    "nulos_comments = reviews_fusionado['comments'].isnull().sum()\n",
    "\n",
    "print(f\"Cantidad de valores nulos en 'comments': {nulos_comments}\")\n",
    "\n",
    "porcentaje_nulos_comments = (nulos_comments / len(reviews_fusionado)) * 100\n",
    "print(f\"Porcentaje de valores nulos en 'comments': {porcentaje_nulos_comments:.2f}%\")\n",
    "\n",
    "# Reemplazar valores nulos en la columna 'comments' con \"no comment\" y verificado\n",
    "reviews_fusionado['comments'].fillna(\"no comment\", inplace=True)\n",
    "nulos_comments_after = reviews_fusionado['comments'].isnull().sum()\n",
    "\n",
    "print(f\"Cantidad de valores nulos en 'comments' después de reemplazar: {nulos_comments_after}\")\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Inicializar lematizador y stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Función de preprocesamiento de texto\n",
    "def preprocess_text(text):\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    # Eliminar caracteres especiales y números\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    # Tokenizar\n",
    "    tokens = word_tokenize(text)\n",
    "    # Eliminar stopwords y lematizar\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    # Unir tokens en una cadena de texto\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Aplicar preprocesamiento a la columna 'comments'\n",
    "reviews_fusionado['comments_clean'] = reviews_fusionado['comments'].apply(preprocess_text)\n",
    "\n",
    "# Verificar los primeros registros después del preprocesamiento\n",
    "print(reviews_fusionado[['comments', 'comments_clean']].head())\n",
    "\n",
    "#  Informe limpieza y transformación reviews_fusionado\n",
    "\n",
    "**Informe de Limpieza y Transformación del Dataset reviews_fusionado**\n",
    "\n",
    "Este informe describe los pasos realizados para la limpieza, transformación y preparación del dataset reviews_fusionado, que resultó de la fusión de los archivos reviews.csv y reviews_2.csv.\n",
    "\n",
    "**1. Conversión de la variable date\n",
    "Acción Considerada:** \n",
    "\n",
    "Inicialmente, se consideró la conversión de la columna date al formato datetime y la creación de una nueva columna date_clean_normalized para representar el número de días desde el 1 de enero de 1970. Esta transformación se planeaba con el fin de asegurar la precisión en la manipulación de fechas y facilitar los análisis temporales y el modelado.\n",
    "\n",
    "**Problemas Encontrados:**\n",
    "\n",
    "Durante el proceso de fusión de los DataFrames listings_fusionados y reviews_fusionado, se detectaron problemas recurrentes relacionados con la columna date_clean_normalized, lo que generó errores y dificultades en la integración de datos. A pesar de los intentos por corregir la fusión, estos problemas persistieron, afectando la estabilidad del proceso.\n",
    "\n",
    "**Decisión Tomada:**\n",
    "\n",
    "Debido a estos problemas, se decidió no realizar la normalización de la columna date y, en su lugar, mantener la columna original, realizándose la conversión del formato object al formato datetime64[ns] utilizando la función pd.to_datetime. Esta decisión permite evitar errores adicionales en el proceso de fusión y asegurar la integridad de los datos. La columna date se incluirá tal cual en el DataFrame final para su uso en análisis posteriores, sin la conversión al formato normalizado.\n",
    "\n",
    "**Resultados:**\n",
    "\n",
    "La columna date se ha conservado en su formato original en el DataFrame final (df_consolidado), pero con el formado adecuado para el análisis sin errores. Este enfoque garantiza la continuidad y estabilidad del proceso de análisis, permitiendo a su vez la flexibilidad para cualquier futura transformación o análisis temporal que se decida realizar.\n",
    "\n",
    "**2. Descripción y Verificación de las Columnas listing_id, id, y reviewer_id**\n",
    "\n",
    "**Acción Realizada:**\n",
    "\n",
    "Se generó una descripción estadística de las columnas listing_id, id, y reviewer_id para entender la distribución y verificar la existencia de valores únicos y duplicados.\n",
    "\n",
    "**Objetivo:**\n",
    "\n",
    "Identificar posibles problemas con duplicados y comprender la distribución de datos en estas columnas.\n",
    "\n",
    "**Resultados:**\n",
    "\n",
    "listing_id: 65,872 valores únicos.\n",
    "id: 1,618,347 valores únicos y ningún duplicado.\n",
    "reviewer_id: 1,374,636 valores únicos, con 243,711 duplicados.\n",
    "\n",
    "**3. Renombrado de la Columna reviewer_id**\n",
    "\n",
    "**Acción Realizada:**\n",
    "\n",
    "La columna reviewer_id fue renombrada a reviewer_id_clean para reflejar que ha sido limpiada.\n",
    "\n",
    "**Objetivo:**\n",
    "\n",
    "Mantener consistencia en la nomenclatura y facilitar el rastreo de las columnas a lo largo del proceso de limpieza.\n",
    "\n",
    "**Resultados:**\n",
    "\n",
    "La columna fue renombrada exitosamente a reviewer_id_clean.\n",
    "\n",
    "**4. Eliminación de la Columna reviewer_name**\n",
    "\n",
    "**Acción Realizada:**\n",
    "\n",
    "Se eliminó la columna reviewer_name dado que su información era redundante al contar con reviewer_id_clean.\n",
    "\n",
    "**Objetivo:**\n",
    "\n",
    "Reducir el volumen de datos innecesarios en el dataset.\n",
    "Resultados: La columna reviewer_name fue eliminada correctamente.\n",
    "\n",
    "**5. Manejo de Valores Nulos en la Columna comments**\n",
    "\n",
    "**Acción Realizada:**\n",
    "\n",
    "Se identificaron y contabilizaron los valores nulos en la columna comments.\n",
    "Decisión Tomada: Los valores nulos fueron reemplazados por el texto \"no comment\" para evitar la eliminación de registros valiosos.\n",
    "Resultados: Todos los valores nulos en comments fueron reemplazados, y la columna quedó lista para el preprocesamiento de texto.\n",
    "\n",
    "**6. Preprocesamiento de la Columna comments**\n",
    "\n",
    "**Acción Realizada:**\n",
    "\n",
    "Se aplicó un proceso de preprocesamiento de texto que incluyó:\n",
    "Conversión a minúsculas.\n",
    "Eliminación de caracteres especiales y números.\n",
    "Tokenización y lematización.\n",
    "Eliminación de stopwords.\n",
    "\n",
    "**Objetivo:**\n",
    "\n",
    "Preparar el texto en comments para su análisis en técnicas de minería de texto.\n",
    "Resultados: Se creó una nueva columna comments_clean que contiene el texto preprocesado y listo para análisis.\n",
    "\n",
    "# Eliminar la columna 'neighbourhood_group' debido a la falta de datos\n",
    "neighbourhoods_clean = neighbourhoods.drop(columns=['neighbourhood_group'])\n",
    "\n",
    "# Verificar que solo queda la columna 'neighbourhood' y que está en buen estado\n",
    "print(\"Columnas restantes en el dataset limpio:\")\n",
    "print(neighbourhoods_clean.columns)\n",
    "\n",
    "# Comprobar si hay valores únicos o inconsistencias en la columna 'neighbourhood'\n",
    "print(\"\\nValores únicos en la columna 'neighbourhood':\")\n",
    "print(neighbourhoods_clean['neighbourhood'].unique())\n",
    "\n",
    "# Comprobar nuevamente la existencia de valores nulos en el dataset limpio\n",
    "nulos_neighbourhoods_clean = neighbourhoods_clean.isnull().sum()\n",
    "print(\"\\nCantidad de valores nulos por columna en 'neighbourhoods_clean':\")\n",
    "print(nulos_neighbourhoods_clean)\n",
    "\n",
    "# Obtener los vecindarios únicos de cada dataset\n",
    "unique_neighbourhoods_in_listings = listings_fusionados['neighbourhood_clean'].unique()\n",
    "unique_neighbourhoods_in_neighbourhoods = neighbourhoods['neighbourhood'].unique()\n",
    "\n",
    "# Comparar los vecindarios de ambos datasets\n",
    "missing_in_listings = set(unique_neighbourhoods_in_neighbourhoods) - set(unique_neighbourhoods_in_listings)\n",
    "missing_in_neighbourhoods = set(unique_neighbourhoods_in_listings) - set(unique_neighbourhoods_in_neighbourhoods)\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"Vecindarios en 'neighbourhoods.csv' que faltan en 'listings_fusionados':\")\n",
    "print(missing_in_listings)\n",
    "\n",
    "print(\"\\nVecindarios en 'listings_fusionados' que faltan en 'neighbourhoods.csv':\")\n",
    "print(missing_in_neighbourhoods)\n",
    "\n",
    "# Informe revisión y limpieza archivo neighbourhoods\n",
    "\n",
    "**Informe de Revisión y Limpieza del Archivo neighbourhoods.csv**\n",
    "\n",
    "**Objetivo**\n",
    "\n",
    "El objetivo de este análisis fue revisar, limpiar y evaluar la relevancia del archivo neighbourhoods.csv en relación con los datos existentes en el dataset listings_fusionados.\n",
    "\n",
    "**Descripción del Archivo**\n",
    "\n",
    "El archivo neighbourhoods.csv contiene información sobre los vecindarios de Londres, con dos columnas:\n",
    "\n",
    "neighbourhood_group: Grupo de vecindarios (con valores nulos).\n",
    "neighbourhood: Nombre del vecindario.\n",
    "\n",
    "**Pasos Realizados**\n",
    "\n",
    "**Revisión Inicial del Archivo:**\n",
    "\n",
    "Se inspeccionaron las primeras filas del archivo para entender la estructura de los datos.\n",
    "Se verificaron las columnas y sus tipos de datos.\n",
    "Se comprobaron valores nulos y duplicados.\n",
    "\n",
    "**Limpieza del Archivo:**\n",
    "\n",
    "Dado que la columna neighbourhood_group contenía únicamente valores nulos, se decidió eliminarla.\n",
    "La columna restante, neighbourhood, no presentaba valores nulos ni duplicados.\n",
    "\n",
    "**Comparación con listings_fusionados:**\n",
    "\n",
    "Se verificó la existencia de la columna neighbourhood_clean en listings_fusionados, la cual ya contenía información sobre los vecindarios.\n",
    "Se realizó una comparación entre los valores únicos de neighbourhood en neighbourhoods.csv y neighbourhood_clean en listings_fusionados.\n",
    "El resultado mostró que no había discrepancias entre los vecindarios listados en ambos archivos.\n",
    "\n",
    "**Conclusión**\n",
    "\n",
    "El archivo neighbourhoods.csv contiene la misma información sobre los vecindarios que ya está presente en la columna neighbourhood_clean de listings_fusionados. Dado que no hay discrepancias ni información adicional relevante en neighbourhoods.csv, no es necesario fusionar este archivo con el dataset principal.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Verificar antes de la fusión\n",
    "print(f\"Dimensiones de listings_fusionados: {listings_fusionados.shape}\")\n",
    "print(f\"Dimensiones de reviews_fusionado: {reviews_fusionado.shape}\")\n",
    "print(listings_fusionados.head())\n",
    "print(\"\\nDimensiones de reviews_fusionado:\", reviews_fusionado.shape)\n",
    "print(\"Primeras filas de reviews_fusionado:\")\n",
    "print(reviews_fusionado.head())\n",
    "\n",
    "unique_ids_listings = listings_fusionados['id'].nunique()\n",
    "unique_ids_reviews = reviews_fusionado['id'].nunique()\n",
    "\n",
    "print(f\"\\nNúmero de ID únicos en listings_fusionados: {unique_ids_listings}\")\n",
    "print(f\"Número de ID únicos en reviews_fusionado: {unique_ids_reviews}\")\n",
    "\n",
    "# Comprobar si todos los IDs en reviews_fusionado están presentes en listings_fusionados y verificar\n",
    "ids_in_both = reviews_fusionado['id'].isin(listings_fusionados['id']).sum()\n",
    "print(f\"Número de ID en reviews_fusionado que están presentes en listings_fusionados: {ids_in_both}\")\n",
    "\n",
    "missing_ids_in_listings = reviews_fusionado[~reviews_fusionado['id'].isin(listings_fusionados['id'])]['id']\n",
    "print(f\"\\nNúmero de ID en reviews_fusionado que no están en listings_fusionados: {len(missing_ids_in_listings)}\")\n",
    "print(\"IDs que no están en listings_fusionados (primeros 10):\")\n",
    "print(missing_ids_in_listings.head(10))\n",
    "\n",
    "# Filtrar los valores válidos en 'listing_id' que sean numéricos, eliminar valores no numéricos, convertir y verificar\n",
    "reviews_fusionado['listing_id'] = pd.to_numeric(reviews_fusionado['listing_id'], errors='coerce')\n",
    "reviews_fusionado = reviews_fusionado.dropna(subset=['listing_id'])\n",
    "reviews_fusionado['listing_id'] = reviews_fusionado['listing_id'].astype('int64')\n",
    "num_ids_validos = reviews_fusionado['listing_id'].nunique()\n",
    "\n",
    "print(f\"Número de IDs únicos válidos en 'reviews_fusionado' después de la limpieza: {num_ids_validos}\")\n",
    "\n",
    "# Comprobar cuántos listing_id en reviews_fusionado están presentes en id de listings_fusionados, id´s coincidentes y no coincidentes y verificar\n",
    "ids_comunes = reviews_fusionado['listing_id'].isin(listings_fusionados['id'])\n",
    "\n",
    "num_ids_coincidentes = ids_comunes.sum()\n",
    "print(f\"Número de IDs de 'reviews_fusionado' que coinciden con 'listings_fusionados': {num_ids_coincidentes}\")\n",
    "\n",
    "num_ids_no_coincidentes = len(reviews_fusionado) - num_ids_coincidentes\n",
    "print(f\"Número de IDs en 'reviews_fusionado' que NO coinciden con 'listings_fusionados': {num_ids_no_coincidentes}\")\n",
    "\n",
    "ids_no_coincidentes = reviews_fusionado.loc[~ids_comunes, 'listing_id'].unique()\n",
    "print(f\"IDs en 'reviews_fusionado' que NO están en 'listings_fusionados' (primeros 10): {ids_no_coincidentes[:10]}\")\n",
    "\n",
    "# Filtrar las filas en reviews_fusionado que tienen IDs coincidentes con listings_fusionados, realizar fusión y verificar\n",
    "reviews_fusionado_filtrado = reviews_fusionado[reviews_fusionado['listing_id'].isin(listings_fusionados['id'])]\n",
    "df_final = listings_fusionados.merge(reviews_fusionado_filtrado[['listing_id', 'date']], left_on='id', right_on='listing_id', how='left')\n",
    "\n",
    "print(f\"Dimensiones del DataFrame final fusionado: {df_final.shape}\")\n",
    "\n",
    "# Guardar el DataFrame final fusionado en un archivo CSV\n",
    "df_final.to_csv('df_final_fusionado.csv', index=False)\n",
    "print(\"Archivo 'df_final_fusionado.csv' creado y guardado exitosamente.\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Verificar los registros sin reseñas y comprobar antigüedad\n",
    "if 'host_since_clean' in df_final.columns:\n",
    "  \n",
    "    ids_sin_resenas = df_final[df_final['date'].isnull()]\n",
    "    ids_sin_resenas_unicos = ids_sin_resenas.drop_duplicates(subset=['id'])\n",
    "    antiguedad_ids_sin_resenas = ids_sin_resenas_unicos[['id', 'host_since_clean']]\n",
    "    antiguedad_ids_sin_resenas['host_since_clean'] = pd.to_datetime(antiguedad_ids_sin_resenas['host_since_clean'], errors='coerce')\n",
    "    print(\"Resumen de antigüedad de IDs sin reseñas:\")\n",
    "    print(antiguedad_ids_sin_resenas['host_since_clean'].describe())\n",
    "\n",
    "    # Ver las primeras filas de la antigüedad de IDs sin reseñas\n",
    "    print(\"Primeras filas de IDs sin reseñas y su antigüedad en la plataforma:\")\n",
    "    print(antiguedad_ids_sin_resenas.head(10))\n",
    "else:\n",
    "    print(\"La columna 'host_since' no está disponible en el DataFrame.\")\n",
    "\n",
    "propiedades_sin_reseñas = df_final[df_final['date'].isnull()]\n",
    "propiedades_sin_reseñas.to_csv('propiedades_sin_reseñas.csv', index=False)\n",
    "\n",
    "# Verificar DataFrame fusionado\n",
    "print(f\"Dimensiones del DataFrame final fusionado: {df_final.shape}\")\n",
    "nulos_date = df_final['date'].isnull().sum()\n",
    "print(f\"Número de valores nulos en la columna 'date' después de la fusión: {nulos_date}\")\n",
    "print(f\"Tipo de dato de la columna 'date': {df_final['date'].dtype}\")\n",
    "print(\"Primeras filas del DataFrame final:\")\n",
    "print(df_final.head())\n",
    "\n",
    "# Informe de la Fusión de los DataFrames listings_fusionados y reviews_fusionad\n",
    "\n",
    "**Informe de la Fusión de los DataFrames listings_fusionados y reviews_fusionado**\n",
    "\n",
    "**1. Contexto y Objetivo**\n",
    "\n",
    "El objetivo de este proceso de fusión es consolidar toda la información relevante de los archivos listings y reviews en un solo DataFrame, denominado df_final. Esta fusión es crucial para un análisis integral de los datos que nos permita explorar patrones, identificar tendencias y construir modelos predictivos en un segundo notebook dedicado al análisis y modelado. La creación de un archivo CSV limpio y consolidado, llamado df_consolidado, facilita la portabilidad de los datos para su análisis en cualquier entorno.\n",
    "\n",
    "**2. Proceso de Fusión**\n",
    "\n",
    "**Cargar y Limpiar los Datos:**\n",
    "\n",
    "Inicialmente, los DataFrames listings_fusionados y reviews_fusionado fueron limpiados y normalizados en el notebook de limpieza de datos.\n",
    "Se crearon columnas adicionales, como date_clean_normalized, para facilitar análisis cronológicos y modelos predictivos.\n",
    "\n",
    "**Fusión de los DataFrames:**\n",
    "\n",
    "Los DataFrames listings_fusionados y reviews_fusionado se fusionaron para combinar las columnas relevantes de ambos archivos.\n",
    "La columna date_clean_normalized del DataFrame reviews_fusionado se añadió a listings_fusionados para asegurar que la información temporal esté disponible para análisis posteriores.\n",
    "El DataFrame resultante, df_final, contiene todas las variables necesarias para realizar un análisis exhaustivo en el siguiente notebook.\n",
    "\n",
    "**Identificación de Propiedades sin Reseñas:**\n",
    "\n",
    "Durante el proceso de fusión, se identificó que algunas propiedades no tenían reseñas asociadas. Para analizarlas de manera específica, se creó un nuevo DataFrame llamado propiedades_sin_reseñas.\n",
    "Este DataFrame contiene todas las propiedades en df_final donde la columna date (que indica la fecha de la reseña) es nula.\n",
    "El análisis de estas propiedades puede ofrecer insights adicionales sobre propiedades que, a pesar de estar listadas en la plataforma, no han recibido feedback de los usuarios.\n",
    "\n",
    "**Creación del Archivo CSV df_consolidado:**\n",
    "\n",
    "Una vez fusionados los datos y añadidas las columnas necesarias, se guardó el DataFrame final como un archivo CSV llamado df_consolidado.csv.\n",
    "Este archivo consolidado servirá como la base de datos para el análisis exploratorio de datos (EDA) y modelado en un segundo notebook, permitiendo que cualquiera pueda cargarlo fácilmente para continuar con el análisis sin necesidad de repetir el proceso de limpieza.\n",
    "\n",
    "**3. Análisis de Propiedades sin Reseñas**\n",
    "\n",
    "Se generó un archivo adicional llamado propiedades_sin_reseñas.csv, que contiene las propiedades sin reseñas.\n",
    "Este análisis permitirá explorar características específicas de estas propiedades, como su antigüedad en la plataforma o su ubicación, y determinar posibles razones por las cuales no han recibido reseñas.\n",
    "\n",
    "**4. Función de la Fusión y Creación del Archivo CSV**\n",
    "\n",
    "La fusión de listings_fusionados y reviews_fusionado, junto con la creación del archivo df_consolidado.csv, es un paso crítico para asegurar que todos los datos relevantes estén disponibles en un formato limpio y listo para su análisis. Este proceso optimiza el flujo de trabajo, separando claramente las etapas de limpieza y análisis, y permite que el análisis sea reproducible y eficiente.\n",
    "\n",
    "El archivo CSV consolidado será utilizado en un segundo notebook, donde se cargarán los datos procesados y se llevará a cabo el análisis exploratorio de datos, así como la construcción de modelos predictivos que podrán ser implementados en aplicaciones prácticas. Además, el análisis específico de las propiedades sin reseñas en propiedades_sin_reseñas.csv proporcionará un enfoque más detallado y dirigido en la comprensión de estos listados.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
